# Setting up a workflow with Docker, GitHub, Travis and AWS

In this post, we are going to set up a workflow to test and deploy automatically a _PHP_ app on _Amazon Web Services Elastic Beanstalk_.

The aim of this article is to set up a basic workflow using a few tools to automate our deployment process. First of all, we need to have an account on each of these platforms: [GitHub](https://github.com), [Docker Hub](https://hub.docker.com), [Amazon Web Services](https://aws.amazon.com) (AWS) and [Travis CI](https://www.travis-ci.org). We also need to have _Docker_ and _Git_ installed on our machine.

Basically, we are going to do the following things: use _Docker_ to create our environment, develop our app, create a repository, commit and push everything to _GitHub_, let _Travis CI_ do the tests, build a _Docker image_ with our code and push everything to _Elastic Beanstalk_ (EB).

## Setting up the environment

To create our environment, we are going to use the _PHP Docker Boilerplate_ from _WebDevOps_. We can clone the repository by simply use:
    
    
    git clone https://github.com/webdevops/php-docker-boilerplate.git our-app

_Cloning webdevops/php-docker-boilerplate_

We will just adjust a very few things. First, in the _Dockerfile.production_ file, at the end of it, we are going to add this simple line:
    
    
    COPY ./app/. /app/

Here, we are copying our app folder instead of sharing it with the host.

If we use another image, we may also need to expose ports and run a command when launching our container like so:
    
    
    EXPOSE 80 
    EXPOSE 443 
    CMD ["supervisord"]

## Initializing our project

We can now go to GitHub and create a new repository. After that, we can make our first commit:
    
    
    git init
    git add .
    git commit –m "First commit"
    git remote add origin https://github.com/user/repository.git
    git push –u origin master

## Initializing our repository

For our example, we create a really simple app using Composer. We must be sure to have the appropriate _.gitignore_ file. A _.dockerignore_ file could also be a good idea:
    
    
    *.md
    .git*
    backup/*
    bin/*
    documentation/*

_.dockerignore file example_

We can now link our _GitHub repository_ to _Travis CI_.

## AWS / EB

We are now going to set a few things on _AWS_. First, we need to create a new application on _EB_. We can name it however we want. Then, we have to create new environment. Here, we need to select “_Docker Multicontainers_” for the _platform option_. We also need to enable _VPC_. We can then proceed.

When it is done, we need to assure that our instance profile can communicate with _Amazon ECS_. To be sure of this, we can attach the _AWSElasticBeanstalkMulticontainerDocker policy_ to it. To set the instance profile, in the _EB dashboard_, we can go to _Configuration &gt; Instances settings_. To attach a policy to a role, in the _IAM dashboard_, we can go to _Roles_, choose the concerned role and then attach the desired policy.

We also need to create a new user. If there is no available group, we have to create on first. When our user is created, we can request credentials. We need to be sure to keep the secret key somewhere because we won’t be able to access it later.

## Information we need

Before going any further, we need to be sure to have the following information:

  * Our app name (APP_NAME)
  * Our Docker username (DOCKER_USERNAME)
  * Our Docker repository name (DOCKER_REPOSITORY)
  * Our Docker password (DOCKER_PASSWORD - secure)
  * Our email adress used with Docker (DOCKER_EMAIL)
  * An image name (IMAGE_NAME)
  * The AWS bucket name (BUCKET_NAME)
  * Our deployment region (DEPLOYMENT_REGION)
  * The deployment bucket (DEPLOYMENT_BUCKET)
  * The deployment environment (DEPLOYMENT_ENV_NAME)
  * The deployment’s id (DEPLOYMENT_ENV - secure)
  * Our AWS access key (AWS_ACCESS_KEY - secure)
  * Our AWS secret key (AWS_SECRET_KEY - secure)



Because we are using _Travis CI_, we can set these values in our _Travis CI_ account in the settings sections of our project or put them in the _.travis.yml_ file and encrypt the private ones with the following command (_Travis CI gem_ need to be installed):
    
    
    travis encrypt SOMEVAR=secretvalue --add

## Docker Configuration

In case we would like to use a private repository on _Docker Hub_, we need to create a file named _.dockercfg_ like so:
    
    
    {
      "https://index.docker.io/v1/": {
        "auth": "",
        "email": ""
      }
    }

_.dockercfg file_

The _Amazon ECS_ container agent will use this file for authentication. Because _Docker_ create a file with another format, we are going to feed this file later with the values that in the files generated by _Docker_.

Now we can create another file named _Dockerrun.aws.json_. This file will be used by _EB_ to deploy our _Docker containers_. Here again, we are going to replace values in this file later.
    
    
    {
     "AWSEBDockerrunVersion": 2,
     "authentication": {
       "bucket": "",
       "key": ".dockercfg"
     },
     "volumes": [
       {
         "name": "storage",
         "host": {
           "sourcePath": "/var/data"
         }
       },
       {
          "name": "app",
          "host": {
            "sourcePath": "/var/app/current/app"
          }
        }
     ],
     "containerDefinitions": [
       {
         "name": "db",
         "image": "mysql:5.6",
         "essential": true,
         "memory": 512,
         "portMappings": [
            {
              "hostPort": 3306,
              "containerPort": 3306
            }
         ]
         "mountPoints": [
           {
             "sourceVolume": "storage",
             "containerPath": "/var/lib/mysql"
           }
         ],
        "environment": [
          {
            "name": "MYSQL_ROOT_PASSWORD",
            "value": "password"
          },
          {
            "name": "MYSQL_DATABASE",
            "value": "my_db"
          }
        ]
       },
       {
         "name": "app",
         "image": ":",
         "essential": true,
         "memory": 256,
         "portMappings": [
           {
             "hostPort": 80,
             "containerPort": 80
           }
         ],
         "links": [
           "db"
         ],
         "mountPoints": [
            {
              "sourceVolume": "app",
              "containerPath": "/var/www/html",
              "readOnly": true
            }
         ]
       }
     ]
    }

_Dockerrun.aws.json file_

## We tell Travis CI what to do

Now, with the _.travis.yml_ file, we can tell _Travis CI_ what to do.
    
    
    sudo: required
    language: php
    python:
    - "3.4"
    - "pypy-5.3.1"
    
    services:
    - docker
    
    before_install:
    # Install dependencies
    - gem update --system
    - sudo apt-get install -y python3.4
    - sudo apt-get install --upgrade -y python-pip
    - sudo apt-get install jq
    - sudo pip install --user virtualenv
    # Create a virtual environment for AWS CLI
    - virtualenv my_py3 --python=/usr/bin/python3.4
    - source my_py3/bin/activate
    - pip install --upgrade awscli
    - pip install --upgrade awsebcli
    # Set AWS information
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY
    - aws configure set aws_secret_access_key $AWS_SECRET_KEY
    - aws configure set default.region $DEPLOYMENT_REGION
    - aws configure set metadata_service_timeout 1200
    - aws configure set metadata_service_num_attempts 3
    - aws configure list
    # Copy the docker-compose.production.yml file to docker-compose.yml file
    - cp docker-compose.production.yml docker-compose.yml
    # Build and create our containers
    - docker-compose up -d
    - docker ps -a
    
    before_script:
    # Install dependencies in the app container
    - docker-compose exec -T app composer self-update
    - docker-compose exec -T app composer install --no-interaction
    - docker-compose exec -T app composer dump-autoload -o 
    
    script:
    # Run unit tests in the app container
    - docker-compose exec -T app vendor/bin/php-cs-fixer fix app --verbose
    - docker-compose exec -T app vendor/bin/phpunit --coverage-clover=coverage.xml
    
    after_success:
    # Stop containers and build our image
    - docker-compose stop
    - docker-compose build --no-cache
    # Send coverage information to Codecov (if needed)
    - bash <(curl -s https://codecov.io/bash)
    # Push image to Docker Hub and update EB environment
    - if [ "$TRAVIS_BRANCH" == "master" ]; then docker login -u="$DOCKER_USERNAME" -p="$DOCKER_PASSWORD";
      docker tag $IMAGE_NAME $DOCKER_USERNAME/$DOCKER_REPOSITORY:$TRAVIS_BUILD_ID; 
      docker push $DOCKER_USERNAME/$DOCKER_REPOSITORY:$TRAVIS_BUILD_ID;
      ./scripts/upload_image_to_elastcbeanstalk.sh $TRAVIS_BUILD_ID $DEPLOYMENT_BUCKET $DEPLOYMENT_ENV $APP_NAME $DEPLOYMENT_REGION $IMAGE_NAME $DEPLOYMENT_ENV_NAME $DOCKER_USERNAME $DOCKER_REPOSITORY $DOCKER_PASSWORD $DOCKER_EMAIL;
      fi
    
    notifications:
      email: false
      
    env:
      global:
      - APP_NAME=app_name
      - DOCKER_USERNAME=docker_username
      - DOCKER_REPOSITORY=docker_repository (here same as app_name)
      - IMAGE_NAME=image_name
      - BUCKET_NAME=bucket_name (here same as app_name)
      - DEPLOYMENT_REGION=us-east-2 (for example)
      - DEPLOYMENT_BUCKET=elasticbeanstalk-us-east-2-xxxxxxxxxxxx
      - DEPLOYMENT_ENV_NAME=env_name
      - DOCKER_EMAIL=docker_email
      - secure: some_secure_value
      - secure: some_secure_value
      - secure: some_secure_value
      - secure: some_secure_value

_.travis.yml file_

Let’s recap what we are doing here:

  * We install the dependencies we need
  * We install _AWS CLI_ in a virtual environment to avoid conflicts and set information
  * Create a _docker-compose.yml_ file from the _docker-compose.production.yml_ file
  * Start our containers
  * Install dependencies in the app container
  * Run tests in the app container
  * Build our image
  * If everything is alright and if we are on master branch, we push the image to _Docker Hub_ and call our script to update the _EB_ environment



## EB environment

In our _.travis.yml_ file, we call a script named _upload_image_to_elastcbeanstalk.sh_. Let’s see what it is:
    
    
    #! /bin/bash
    
    # Variables
    DOCKER_TAG=$1
    DOCKERRUN_FILE="Dockerrun.aws.json"
    DOCKERCFG=".dockercfg"
    DOCKER_CONFIG="/home/travis/.docker/config.json"
    EB_BUCKET=$2
    EB_ENV=$3
    PREFIX="deploy/$DOCKER_TAG"
    APP_NAME=$4
    DEPLOYMENT_REGION=$5
    IMAGE_NAME=$6
    DEPLOYMENT_ENV_NAME=$7
    DOCKER_USERNAME=$8
    DOCKER_REPOSITORY=$9
    DOCKER_PASSWORD=${10}
    DOCKER_EMAIL=${11}
    DOCKER_IMAGE="$DOCKER_USERNAME/$DOCKER_REPOSITORY"
    
    # Generate dockercfg
    echo "::::: Creating .dockercfg file :::::"
    
    DOCKER_AUTH=($(sudo jq -r '.auths["https://index.docker.io/v1/"].auth' $DOCKER_CONFIG))
    
    cat "$DOCKERCFG" \
      | sed 's||'$DOCKER_AUTH'|g' \
      | sed 's||'$DOCKER_EMAIL'|g' \
      > $DOCKERCFG
    
    sleep 30
    
    aws s3 cp $DOCKERCFG s3://$EB_BUCKET/.dockercfg
    
    sleep 30
    
    echo "::::: Creating Dockerrun.aws.json file :::::"
    
    # Replace vars in the DOCKERRUN_FILE 
    cat "$DOCKERRUN_FILE" \
      | sed 's||'$EB_BUCKET'|g' \
      | sed 's||'$DOCKER_IMAGE'|g' \
      | sed 's||'$DOCKER_TAG'|g' \
      > $DOCKERRUN_FILE
    
    sleep 30
    
    aws s3 cp $DOCKERRUN_FILE s3://$EB_BUCKET/$PREFIX/$DOCKERRUN_FILE
    sleep 30
    
    echo "::::: Creating new Elastic Beanstalk version :::::"
    
    # Run aws command to create a new EB application with label
    aws elasticbeanstalk create-application-version \
    	--region=$DEPLOYMENT_REGION \
    	--application-name $APP_NAME \
      --version-label $DOCKER_TAG \
    	--source-bundle S3Bucket=$EB_BUCKET,S3Key=$PREFIX/$DOCKERRUN_FILE
      
    sleep 30  
    
    echo "::::: Updating Elastic Beanstalk environment :::::"
    
    aws elasticbeanstalk update-environment \
      --environment-id $EB_ENV \
      --environment-name $DEPLOYMENT_ENV_NAME \
      --application-name $APP_NAME \
      --version-label $DOCKER_TAG
    
    echo "::::: Removing file :::::"
    
    sleep 30  
    rm $DOCKERCFG
    rm $DOCKERRUN_FILE

_upload_image_to_elastcbeanstalk.sh file_

Let’s recap what we are doing here:

  * We parse the file _/home/travis/.docker/config.json_ and extract the “_auth_” value from it
  * We replace values in the _.dockercfg_ and upload it on _AWS S3_
  * We replace values in _Dockerrun.aws.json_ file and upload it on _AWS S3_
  * We create a new version for the application
  * We update the _EB_ environment
  * We remove the configuration files



## The end!

If everything is alright, our app will be deployed on _AWS EB_ by the next push.

## Test locally and fix problems

If we need to run our image locally, we can do the following thing:
    
    
    docker pull username/repository:tag
    docker run --expose 80 -p 80:80 -it username/repository:tag

Or if we need to use _sh_:
    
    
    docker run -ti username/repository:tag sh

To get console output from the instance, we can use the following command (_awscli_ has to be installed; we may need to run _aws configure_ first):
    
    
    aws ec2 get-console-output --instance-id instance_id

_Getting console output_

We may also want to connect to our instance using _SSH_. To do so, we just have to use the following command (_awsebcli_ has to be installed; we may need to run _eb init_ first):
    
    
    eb ssh --setup // If needed
    eb ssh environment-name

_Using SSH_

We can retrieve and display _logs_ like so:
    
    
    eb logs

_Displaying logs_

## Further configuration and optimization

We may want to extend our configuration. For that, we can place some _.config_ files in the _.ebextentions_ folder. For example, we may need a script to clean unused images:
    
    
    option_settings:
      - namespace: aws:elasticbeanstalk:command
        option_name: Timeout
        value: 1200
    
    commands:
      docker_clean_containers:
        command: docker rm -v $(docker ps -a -q)
        ignoreErrors: true
      docker_clean_images:
        command: docker rmi $(docker images -q)
        ignoreErrors: true

_Cleaning unused images - .ebextentions/docker.config_

In our example, we also use a _MYSQL_ container for our database. Using _AWS RDS_ instead can be a reliable choice for production.

## Conclusion

Setting up this whole process may seem to be quite a headache that involves many different tools. On the other hand, we have the opportunity to automate our deployment process while preserving control and security.
